{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alepi\\Documents\\projects\\personal\\repos\\toy_ml_pipeline\\mlpipe\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "cfg = {\n",
    "    'algorithm': 'chained_hgb',\n",
    "    'sklearn_model': HistGradientBoostingRegressor,\n",
    "    'time_format' : '%Y-%m-%d',\n",
    "    'date': dt.datetime.today().strftime('%Y-%m-%d'),\n",
    "    'cv_folds': 4,\n",
    "    'n_jobs': 4,\n",
    "    'seed': 42,\n",
    "    'company':'GOOG',\n",
    "    'future': 7,\n",
    "    'past': 90\n",
    "}\n",
    "\n",
    "cfg['modelname'] = f\"{cfg['algorithm']}_{cfg['date']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# dowload stock price data\n",
    "today = dt.datetime.now()\n",
    "start = today - dt.timedelta(days=365*10)\n",
    "\n",
    "df = yf.download(cfg['company'], start = start.strftime(cfg['time_format']), end=today.strftime(cfg['time_format']))\n",
    "df = df[['Close']].rename(columns={'Close': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and apply window function\n",
    "def window_df(data: pd.DataFrame, past: int, future: int) -> pd.DataFrame:\n",
    "    '''Shift and name values in order to have past days of observations as features and future days as targets in a preprocessed dataframe'''\n",
    "\n",
    "    df = data.copy()\n",
    "    for i in range(0, past):\n",
    "        df[f'y_t-{i}'] = df['y'].shift(i)\n",
    "    for j in range(1, future+1):\n",
    "        df[f'y_t+{j}'] = df['y'].shift(-j)\n",
    "\n",
    "    df.drop(columns=['y'], inplace=True)\n",
    "    df.rename(columns={'y_t-0':'y_t'}, inplace=True)\n",
    "    #df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "proc_df = window_df(df, past=cfg['past'], future=cfg['future'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective: predict GOOG stock price 7 days ahead using past 90-day history\n",
      "Train features range from 2013-09-10 to 2023-04-18 (2418 data points) and targets look 7 steps ahead\n",
      "We predict 7 days ahead using data from 2023-04-19 (we predict this week as a backtesting)\n",
      "Lastly we will train using all vailable data and make predictions a week from today\n"
     ]
    }
   ],
   "source": [
    "# final preprocessing\n",
    "\n",
    "# get feature and target names\n",
    "x_cols = [col for col in proc_df.columns if col.startswith('y_t-') or col=='y_t']\n",
    "y_cols = [col for col in proc_df.columns if col.startswith('y_t+')]\n",
    "\n",
    "# save most recent features then remove missing values\n",
    "x_today = proc_df[x_cols][-1:]\n",
    "proc_df.dropna(inplace=True)\n",
    "\n",
    "# pop out last observation as test set\n",
    "x = proc_df[x_cols][:-1]\n",
    "y = proc_df[y_cols][:-1]\n",
    "\n",
    "x_test = proc_df[x_cols][-1:]\n",
    "y_test = proc_df[y_cols][-1:]\n",
    "\n",
    "# print info\n",
    "print(f'Objective: predict {cfg[\"company\"]} stock price {cfg[\"future\"]} days ahead using past {cfg[\"past\"]}-day history')\n",
    "print(f'Train features range from {x.index[0].strftime(cfg[\"time_format\"])} to {x.index[-1].strftime(cfg[\"time_format\"])} ({x.shape[0]} data points) and targets look {cfg[\"future\"]} steps ahead')\n",
    "print(f'We predict {cfg[\"future\"]} days ahead using data from {x_test.index[0].strftime(cfg[\"time_format\"])} (we predict this week as a backtesting)')\n",
    "print('Lastly we will train using all vailable data and make predictions a week from today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross validation strategy\n",
    "cv = KFold(n_splits = cfg['cv_folds'], shuffle = True, random_state = cfg['seed'])\n",
    "\n",
    "# fixed params\n",
    "fixed_params = {\n",
    "        'loss': 'squared_error',\n",
    "        'scoring': 'neg_mean_absolute_percentage_error',\n",
    "        'verbose': 0,\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': .1,\n",
    "        'n_iter_no_change': 15,\n",
    "        'random_state': 42 \n",
    "    }\n",
    "\n",
    "# objective function for optimization\n",
    "def objective(trial):\n",
    "    \n",
    "    # trial parameters\n",
    "    tuning_params = {\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-8, 10.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "    }\n",
    "    params = {**fixed_params, **tuning_params}\n",
    "\n",
    "    # train and score with cv\n",
    "    model = RegressorChain(base_estimator=HistGradientBoostingRegressor(**params))\n",
    "    cv_res = cross_validate(\n",
    "        estimator = model, \n",
    "        X = x, \n",
    "        y = y,\n",
    "        scoring = {\n",
    "            'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False, multioutput='uniform_average'),\n",
    "            #'mae': make_scorer(mean_absolute_error, greater_is_better=False, multioutput='uniform_average'),\n",
    "            #'rmse': make_scorer(mean_squared_error, greater_is_better=False, multioutput='uniform_average', squared=False)\n",
    "        },\n",
    "        cv = cv,\n",
    "        n_jobs= cfg['n_jobs']\n",
    "    )\n",
    "\n",
    "    # return mean cv score \n",
    "    return -np.mean(cv_res['test_mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-01 21:27:55,949]\u001b[0m A new study created in memory with name: chained_hgb_2023-05-01_optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chained_hgb_2023-05-01 optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-01 21:28:14,355]\u001b[0m Trial 0 finished with value: 0.022484966774880228 and parameters: {'max_iter': 437, 'learning_rate': 0.28767857660247903, 'l2_regularization': 7.31993942079411, 'max_depth': 7}. Best is trial 0 with value: 0.022484966774880228.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# create study\n",
    "sampler = optuna.samplers.TPESampler(seed=cfg['seed'])\n",
    "max_trials = 1\n",
    "time_limit = 3600 * 0.5\n",
    "\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    study_name= f\"{cfg['modelname']}_optimization\",\n",
    "    direction='minimize')\n",
    "\n",
    "# perform optimization\n",
    "print(f\"Starting {cfg['modelname']} optimization...\")\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials = max_trials,\n",
    "    timeout = time_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 1\n",
      "Best score: 0.022484966774880228\n",
      "Best trial parameters:\n",
      "\tloss: squared_error\n",
      "\tscoring: neg_mean_absolute_percentage_error\n",
      "\tverbose: 0\n",
      "\tearly_stopping: True\n",
      "\tvalidation_fraction: 0.1\n",
      "\tn_iter_no_change: 15\n",
      "\trandom_state: 42\n",
      "\tmax_iter: 437\n",
      "\tlearning_rate: 0.28767857660247903\n",
      "\tl2_regularization: 7.31993942079411\n",
      "\tmax_depth: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>sklearn_model</th>\n",
       "      <th>date</th>\n",
       "      <th>modelname</th>\n",
       "      <th>params_names</th>\n",
       "      <th>loss</th>\n",
       "      <th>scoring</th>\n",
       "      <th>verbose</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>validation_fraction</th>\n",
       "      <th>n_iter_no_change</th>\n",
       "      <th>random_state</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_regularization</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chained_hgb</td>\n",
       "      <td>&lt;class 'sklearn.ensemble._hist_gradient_boosti...</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>chained_hgb_2023-05-01</td>\n",
       "      <td>[loss, scoring, verbose, early_stopping, valid...</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>neg_mean_absolute_percentage_error</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>437</td>\n",
       "      <td>0.287679</td>\n",
       "      <td>7.319939</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     algorithm                                      sklearn_model        date   \n",
       "0  chained_hgb  <class 'sklearn.ensemble._hist_gradient_boosti...  2023-05-01  \\\n",
       "\n",
       "                modelname                                       params_names   \n",
       "0  chained_hgb_2023-05-01  [loss, scoring, verbose, early_stopping, valid...  \\\n",
       "\n",
       "            loss                             scoring  verbose  early_stopping   \n",
       "0  squared_error  neg_mean_absolute_percentage_error        0            True  \\\n",
       "\n",
       "   validation_fraction  n_iter_no_change  random_state  max_iter   \n",
       "0                  0.1                15            42       437  \\\n",
       "\n",
       "   learning_rate  l2_regularization  max_depth  \n",
       "0       0.287679           7.319939          7  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization results\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best score: {study.best_value}\")\n",
    "\n",
    "best_params = {**fixed_params, **study.best_trial.params}\n",
    "print(\"Best trial parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"\\t{k}: {v}\")\n",
    "\n",
    "# save results\n",
    "cfg_df = pd.DataFrame([{'algorithm': cfg['algorithm'], 'sklearn_model': cfg['sklearn_model'], 'date': cfg['date'], 'modelname': cfg['modelname'], 'params_names': list(best_params.keys())}])\n",
    "params_df = pd.DataFrame([best_params])\n",
    "\n",
    "exp_df = pd.concat([cfg_df, params_df], axis=1)\n",
    "exp_df.to_csv(f'./{cfg[\"modelname\"]}_exp.csv', index=False)\n",
    "exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters:\n",
      "\tloss: squared_error\n",
      "\tscoring: neg_mean_absolute_percentage_error\n",
      "\tverbose: 0\n",
      "\tearly_stopping: True\n",
      "\tvalidation_fraction: 0.1\n",
      "\tn_iter_no_change: 15\n",
      "\trandom_state: 42\n",
      "\tmax_iter: 437\n",
      "\tlearning_rate: 0.28767857660247903\n",
      "\tl2_regularization: 7.31993942079411\n",
      "\tmax_depth: 7\n",
      "Average of metric across this weeks predictions:\n",
      "\tmape 0.0265\n",
      "\tmae 2.7373\n",
      "\trmse 2.7373\n"
     ]
    }
   ],
   "source": [
    "# load best params\n",
    "best_params = exp_df[exp_df['params_names'].values[0]].to_dict(orient='records')[0]\n",
    "print(\"Final parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"\\t{k}: {v}\")\n",
    "\n",
    "# backtesting\n",
    "model = RegressorChain(base_estimator=HistGradientBoostingRegressor(**best_params))\n",
    "model.fit(x, y)\n",
    "yp = model.predict(x_test)\n",
    "\n",
    "# calculate and print metrics\n",
    "mape = mean_absolute_percentage_error(yp, y_test, multioutput='uniform_average')\n",
    "mae = mean_absolute_error(yp, y_test, multioutput='uniform_average')\n",
    "rmse = mean_squared_error(yp, y_test, multioutput='uniform_average', squared=False)\n",
    "print(f'Average of metric across this weeks predictions:\\n\\tmape {mape:.4f}\\n\\tmae {mae:.4f}\\n\\trmse {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.scatter(yp[0], y_test.T.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[108.31911676, 108.59025453, 108.49575155, 108.22518639,\n",
       "        107.6251103 , 109.03460313, 110.41794442]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on all data and predict a week from today\n",
    "x_all, y_all= pd.concat([x, x_test]), pd.concat([y, y_test])\n",
    "\n",
    "model = RegressorChain(base_estimator=HistGradientBoostingRegressor(**best_params))\n",
    "model.fit(x, y)\n",
    "\n",
    "yp = model.predict(x_today)\n",
    "yp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
